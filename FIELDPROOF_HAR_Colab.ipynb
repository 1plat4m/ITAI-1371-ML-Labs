{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53a680c3",
   "metadata": {},
   "source": [
    "# FIELDPROOF™ ML Prototype (Colab)\n",
    "### Human Activity Recognition (HAR) → Sensor-Verified Task Execution\n",
    "\n",
    "This notebook builds a **FIELDPROOF-style** prototype using a **public wearable sensor dataset** (UCI HAR).\n",
    "We train several **course-level ML models**, measure results, and compare to a published baseline.\n",
    "\n",
    "**Dataset:** UCI Human Activity Recognition Using Smartphones (30 subjects, 6 activities, 561 features, 10,299 windows)  \n",
    "**Goal (course prototype):** Verify whether sensor signals match a required task (e.g., *Walking*) and classify activity states.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6635632",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install pandas numpy scikit-learn matplotlib joblib\n",
    "print('✅ Installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd9f65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, zipfile, urllib.request, pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"✅ Imports ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28f50b0",
   "metadata": {},
   "source": [
    "## 1) Download the public dataset (UCI HAR)\n",
    "\n",
    "We download the official zip and load the provided train/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893534bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "zip_url = \"https://archive.ics.uci.edu/static/public/240/human+activity+recognition+using+smartphones.zip\"\n",
    "zip_path = DATA_DIR / \"uci_har.zip\"\n",
    "extract_dir = DATA_DIR / \"UCI HAR Dataset\"\n",
    "\n",
    "if not zip_path.exists():\n",
    "    print(\"Downloading dataset...\")\n",
    "    urllib.request.urlretrieve(zip_url, zip_path)\n",
    "    print(\"✅ Downloaded:\", zip_path)\n",
    "\n",
    "if not extract_dir.exists():\n",
    "    print(\"Extracting...\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        z.extractall(DATA_DIR)\n",
    "    print(\"✅ Extracted to:\", extract_dir)\n",
    "\n",
    "extract_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36ea170",
   "metadata": {},
   "source": [
    "## 2) Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946f461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_uci_har(base_dir: pathlib.Path):\n",
    "    base = base_dir / \"UCI HAR Dataset\"\n",
    "    X_train = pd.read_csv(base / \"train\" / \"X_train.txt\", sep=r\"\\s+\", header=None)\n",
    "    y_train = pd.read_csv(base / \"train\" / \"y_train.txt\", sep=r\"\\s+\", header=None)[0]\n",
    "    X_test  = pd.read_csv(base / \"test\" / \"X_test.txt\",  sep=r\"\\s+\", header=None)\n",
    "    y_test  = pd.read_csv(base / \"test\" / \"y_test.txt\",  sep=r\"\\s+\", header=None)[0]\n",
    "\n",
    "    act_map = pd.read_csv(base / \"activity_labels.txt\", sep=r\"\\s+\", header=None, names=[\"id\",\"label\"])\n",
    "    id_to_label = dict(zip(act_map[\"id\"], act_map[\"label\"]))\n",
    "\n",
    "    return X_train, y_train.map(id_to_label), X_test, y_test.map(id_to_label), act_map\n",
    "\n",
    "X_train, y_train, X_test, y_test, act_map = load_uci_har(DATA_DIR)\n",
    "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)\n",
    "act_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e03e49",
   "metadata": {},
   "source": [
    "## 3) Class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede648b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1469d1",
   "metadata": {},
   "source": [
    "## 4) Train & compare models (course-level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02039682",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"LogReg\": Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(max_iter=2000))]),\n",
    "    \"KNN\": Pipeline([(\"scaler\", StandardScaler()), (\"clf\", KNeighborsClassifier(n_neighbors=15))]),\n",
    "    \"LinearSVM\": Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LinearSVC())]),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=400, random_state=42, n_jobs=-1),\n",
    "}\n",
    "\n",
    "results = []\n",
    "trained = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    results.append({\n",
    "        \"model\": name,\n",
    "        \"accuracy\": accuracy_score(y_test, pred),\n",
    "        \"f1_macro\": f1_score(y_test, pred, average=\"macro\")\n",
    "    })\n",
    "    trained[name] = model\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"accuracy\", ascending=False).reset_index(drop=True)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4c9cda",
   "metadata": {},
   "source": [
    "## 5) Best model report + confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fe9c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_name = results_df.loc[0, \"model\"]\n",
    "best_model = trained[best_name]\n",
    "best_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"Best model:\", best_name)\n",
    "print(classification_report(y_test, best_pred))\n",
    "\n",
    "labels = list(act_map[\"label\"])\n",
    "cm = confusion_matrix(y_test, best_pred, labels=labels)\n",
    "cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "cm_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c23d0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(cm, interpolation=\"nearest\")\n",
    "plt.title(f\"Confusion Matrix — {best_name}\")\n",
    "plt.xticks(range(len(labels)), labels, rotation=45, ha=\"right\")\n",
    "plt.yticks(range(len(labels)), labels)\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27054a3c",
   "metadata": {},
   "source": [
    "## 6) FIELDPROOF mapping: binary task verification (valid vs invalid)\n",
    "\n",
    "Example: required task = **WALKING**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0e587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_label = \"WALKING\"\n",
    "y_train_bin = (y_train == task_label).astype(int)\n",
    "y_test_bin  = (y_test == task_label).astype(int)\n",
    "\n",
    "bin_model = Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(max_iter=2000))])\n",
    "bin_model.fit(X_train, y_train_bin)\n",
    "bin_pred = bin_model.predict(X_test)\n",
    "\n",
    "print(\"Binary verification for task:\", task_label)\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test_bin, bin_pred), 4))\n",
    "print(\"F1:\", round(f1_score(y_test_bin, bin_pred), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa18e1b",
   "metadata": {},
   "source": [
    "## 7) Compare to published baseline (quick benchmark note)\n",
    "\n",
    "Classic UCI HAR baselines often report ~**96%** multiclass accuracy with SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afe5645",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = float(results_df.loc[0, \"accuracy\"])\n",
    "paper_reported_acc = 0.96  # benchmark value often cited for multiclass SVM on UCI HAR\n",
    "\n",
    "print(\"Your best model:\", best_name)\n",
    "print(\"Your best accuracy:\", round(best_acc, 4))\n",
    "print(\"Published baseline (example):\", paper_reported_acc)\n",
    "print(\"Delta (yours - baseline):\", round(best_acc - paper_reported_acc, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98b7660",
   "metadata": {},
   "source": [
    "## 8) Save best model (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f0df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "ART = pathlib.Path(\"artifacts\")\n",
    "ART.mkdir(exist_ok=True)\n",
    "out = ART / f\"best_model_{best_name}.joblib\"\n",
    "joblib.dump(best_model, out)\n",
    "print(\"✅ Saved:\", out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d8fef3",
   "metadata": {},
   "source": [
    "## ✅ Done\n",
    "- Public dataset loaded\n",
    "- Multiple ML models compared\n",
    "- FIELDPROOF-style verification demo included\n",
    "- Ready for your course submission + write-up\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
